{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smiley Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emot.core import emot\n",
    "from googletrans import Translator  # version 3.1.0a0 use: pip install googletrans==3.1.0a0\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to get the right english translation \n",
    "from the 'mean' value in the dictionary returned by the <code>emoticon()</code> function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. '^[^\\s,]+,': match the first word before the first comma\n",
    "    - ^:        start of the string\n",
    "    - [^\\s,]+:  match any character that is not a space or a comma, one or more times\n",
    "\n",
    "2. '^[^,]+,\\s*(\\w+)': match the first word after the first comma if there are multiple words before the comma\n",
    "    - ^:        start of the string\n",
    "    - [^,]+:    one or more characters that are not commas, followed by a comma\n",
    "    - \\s*:      zero or more whitespace characters (e.g., spaces or tabs)\n",
    "    - (\\w+):    one or more word characters (e.g., letters, digits, or underscores), captured in a group\n",
    "    \n",
    "3. '^\\s*([\\w\\s]+?)\\s*or\\b': match the first words before the word 'or'\n",
    "    - ^:            start of the string\n",
    "    - \\s*:          zero or more whitespace characters (e.g., spaces or tabs)\n",
    "    - ([\\w\\s]+?):   one or more word characters (e.g., letters, digits, or underscores), or whitespace characters, captured in a non-greedy group\n",
    "    - \\s*:          zero or more whitespace characters\n",
    "    - or\\b:         the word \"or\", followed by a word boundary (to avoid matching words like \"order\" or \"orange\")\n",
    "'''\n",
    "def replacer(meanings):\n",
    "    regex = re.compile(r'^[^\\s,]+,')\n",
    "    match = regex.match(meanings)\n",
    "    if match:\n",
    "        return match.group(0)[:-1]\n",
    "    \n",
    "    regex = re.compile(r'^[^,]+,\\s*(\\w+)')\n",
    "    match = regex.match(meanings)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    regex = re.compile(r'^\\s*([\\w\\s]+?)\\s*or\\b')\n",
    "    match = regex.match(meanings)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    return meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_emoticons(text):\n",
    "    emotions = emot().emoticons(text)\n",
    "    correction = 0\n",
    "    for i, location in enumerate(emotions['location']):\n",
    "        emoticon = emotions['value'][i]\n",
    "        start = location[0] + correction\n",
    "        end = location[1] + correction\n",
    "        meaning = emotions['mean'][i]\n",
    "        replacement = replacer(meaning)\n",
    "        text = text[:start] + replacement + text[end:]\n",
    "        correction += len(replacement) - len(emoticon)     # correction for the length of the emoticon\n",
    "    return text\n",
    "test = \"Hoi Andre i bi nöd bös :/, :D, :P, :), :(, :)), :))), :-)\"\n",
    "translator = Translator()\n",
    "translator.translate(translate_emoticons(test), src='de', dest='en').text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate swiss german to standard german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to install googletrans version 3.1.0a0\n",
    "def translate_swiss_german(text):\n",
    "    translator = Translator()\n",
    "    return translator.translate(translator.translate(text, src='de', dest='en').text, src='en', dest='de').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texte = ['Das isch en super Sach!', \n",
    "         'I lieb di au!', \n",
    "         \"Chrischtbaumschmuck, Brunsli, Nusshüüfeli, Haferflockeguetzli, Zimetstärn hani gärn, Mailänderli au, Aenisguetzli, Chrischtchindli, es lüütet es Glöögli, Schtilli Nacht, Schternschnuppe,s' Jesuschindli liit i de Chrippe, äs isch zu euch Mänsche uf d'Aerde abe cho\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texte:\n",
    "    print(text)\n",
    "    print(translate_swiss_german(text))\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time the function with multiple Emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r 5 -n 10 translate_emoticons(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r 5 -n 10 translate_swiss_german(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restliche Emoticons von hand entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :-D, <3, "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chatmania & facebook merge\n",
    "emoticons übersetzen\n",
    "2 files: facebook_english, facebook_deutsch (mit der jeweiligen sprache übersetzen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACEBOOK_PATH = \"facebook_dataset/translated.csv\"\n",
    "SENTIMENT_PATH = \"dataset/sentiment.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the comment data\n",
    "comments_df = pd.read_csv(FACEBOOK_PATH)\n",
    "# Load the sentiment counts data\n",
    "sentiment_df = pd.read_csv(SENTIMENT_PATH)\n",
    "# Merge the dataframes based on the sentence_id column\n",
    "merged_df = pd.merge(comments_df, sentiment_df, on='sentence_id')\n",
    "merged_df[\"sentence_text\"] = merged_df[\"sentence_text\"].str.lower().replace('[^\\w\\s]','')\n",
    "print(comments_df.shape, sentiment_df.shape)\n",
    "merged_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_df[\"sentence_text\"].values\n",
    "y = merged_df[['neut', 'neg', 'pos']].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_stop_words = stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=german_stop_words)\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model with naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "multi_clf = MultiOutputClassifier(clf, n_jobs=-1)\n",
    "multi_clf.fit(X_train_counts, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Hoi du schwuli sau\"]\n",
    "new_sentence_counts = vectorizer.transform(text)\n",
    "prediciton = multi_clf.predict(new_sentence_counts)\n",
    "sentiments = np.array(['neutral', 'negative', 'positive'])\n",
    "most_likely_sentiment = sentiments[np.argmax(prediciton)]\n",
    "print(f'Text: {text}, Sentiment: {most_likely_sentiment}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the sentiment probabilities for the test set\n",
    "y_pred = multi_clf.predict(X_test_counts)\n",
    "multi_clf.classes_ = ['neutral', 'negative', 'positive']\n",
    "# calculate accuracy, precision, recall, and F1 score for each label\n",
    "for i in range(y_test.shape[1]):\n",
    "    label = multi_clf.classes_[i]\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test[:,i], y_pred[:,i])}\")\n",
    "    print(f\"Precision: {precision_score(y_test[:,i], y_pred[:,i], average='weighted', zero_division=0)}\")\n",
    "    print(f\"Recall: {recall_score(y_test[:,i], y_pred[:,i], average='weighted')}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test[:,i], y_pred[:,i], average='weighted')}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V2 merge, translate chatmania & facebook.full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emot.core import emot\n",
    "from googletrans import Translator  # version 3.1.0a0 use: pip install googletrans==3.1.0a0\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACEBOOK_PATH = \"facebook_dataset/facebook.full.csv\"\n",
    "CHATMANIA_PATH = \"dataset/chatmania.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacer(meanings):\n",
    "    regex = re.compile(r'^[^\\s,]+,')\n",
    "    match = regex.match(meanings)\n",
    "    if match:\n",
    "        return match.group(0)[:-1]\n",
    "    \n",
    "    regex = re.compile(r'^[^,]+,\\s*(\\w+)')\n",
    "    match = regex.match(meanings)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    regex = re.compile(r'^\\s*([\\w\\s]+?)\\s*or\\b')\n",
    "    match = regex.match(meanings)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    return meanings\n",
    "\n",
    "def translate_emoticons(text):\n",
    "    emotions = emot().emoticons(text)\n",
    "    correction = 0\n",
    "    for i, location in enumerate(emotions['location']):\n",
    "        emoticon = emotions['value'][i]\n",
    "        start = location[0] + correction\n",
    "        end = location[1] + correction\n",
    "        meaning = emotions['mean'][i]\n",
    "        replacement = replacer(meaning)\n",
    "        text = text[:start] + replacement + text[end:]\n",
    "        correction += len(replacement) - len(emoticon)\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load both Dataframes and drop unnecessairy cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_facebook = pd.read_csv(FACEBOOK_PATH)\n",
    "df_facebook = df_facebook[['sentence_id', 'sentence_text']]\n",
    "df_chatmania = pd.read_csv(CHATMANIA_PATH)\n",
    "df_facebook.shape, df_chatmania.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if the sentence IDs are unique and the values can be appended (no duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sentence_ids = df_facebook['sentence_id'].unique()\n",
    "unique_sentence_ids_chatmania = df_chatmania['sentence_id'].unique()\n",
    "unique_sentence_ids.shape, unique_sentence_ids_chatmania.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are ids in both datasets\n",
    "any([sentence_id in unique_sentence_ids_chatmania for sentence_id in unique_sentence_ids])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([df_facebook, df_chatmania], ignore_index=True)\n",
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate emoticon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "for i, row in tqdm(merged_df.iterrows(), total=merged_df.shape[0]):\n",
    "    text = row['sentence_text']\n",
    "    merged_df.at[i, 'sentence_text'] = translate_emoticons(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df = merged_df.copy().assign(translate_success=False)\n",
    "german_df = merged_df.copy().assign(translate_success=False)\n",
    "translator = Translator()\n",
    "failed_translations_indexes = []\n",
    "for i, row in tqdm(merged_df.iterrows(), total=merged_df.shape[0]):\n",
    "    try:\n",
    "        translated_engl = translator.translate(row['sentence_text'], dest='en').text\n",
    "        english_df.loc[i] = [row['sentence_id'], translated_engl, True]\n",
    "        translated_ger = translator.translate(translated_engl, dest='de').text\n",
    "        german_df.loc[i] = [row['sentence_id'], translated_ger, True]\n",
    "    except:\n",
    "        failed_translations_indexes.append(i)\n",
    "        english_df.loc[i] = [row['sentence_id'], row['sentence_text'], False]\n",
    "        german_df.loc[i] = [row['sentence_id'], row['sentence_text'], False]\n",
    "        continue\n",
    "print(f\"Failed translations: {len(failed_translations_indexes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df.to_csv(\"facebook_dataset/english.csv\", index=False)\n",
    "german_df.to_csv(\"facebook_dataset/german.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while failed_translations_indexes:\n",
    "    last_index = failed_translations_indexes.pop()\n",
    "    # try translating the sentence again\n",
    "    row = merged_df.loc[last_index]\n",
    "    try:\n",
    "        translated_engl = translator.translate(row['sentence_text'], dest='en').text\n",
    "        english_df.loc[i] = [row['sentence_id'], translated_engl, True]\n",
    "        translated_ger = translator.translate(translated_engl, dest='de').text\n",
    "        german_df.loc[i] = [row['sentence_id'], translated_ger, True]\n",
    "    except:\n",
    "        print(f\"Failed to translate: Row {last_index}\")\n",
    "        failed_translations_indexes.append(last_index)\n",
    "        continue\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
